G. Copy/Shuffle Phase Tuning
       1. If Reduce phase doesn’t complete copying Map outputs soon after all the Map tasks 
          are executed  poorly configured cluster.
       2. What causes a slow progressing copy phase?
            (1) The maximum number of parallel map-output copier threads
                    mapred.reduce.parallel.copies in mapred-site.xml
                    default = 5
                    the limiting factor for the throughput of copy operation
             (2) The maximum number of  worker threads at TaskTracker level
                    serve Map outputs to the reducers
                    tasktracker.http.threads
                    default = 40
              (3) For further fune-tuning
                    dfs.datanode.handler.count in hdfs-site.xml
                    dfs.namenode.handler.count in hdfs-site.xml
                    mapred.job.tracker.handler.count in mapred-site.xml
              (4) Avoiding disk spills on the Reduce side could speed up the copy phase.
              (5) Network hardware related bottlenecks.

H. Reduce-side Spills
        Reduce phase can be crucial in influencing the total execution time
        Reduce phase is more network intensive and could be more IO intensive
        The output generated by large number of Map tasks needs to be copied, aggregated/merged, 
         processed, and all this data needs to be written back to the HDFS.
        1. Once Map tasks start completing their work:
             (1) Map output gets sorted and partitioned per Reduced
             (2) Map output is written to the disks of the TaskTracker node
             (3) Map partitions are copied over to the appropriate Reducer TaskTrackers
               A buffer will hold the Map output data if big enough
                   * mapred.job.shuffle.input.buffer.percent in mapred-site.xml
                   * default = 0.7; means 70% of the Reduce JVM heap space will be served for
                     storing copied Map output data
               Otherwise, the Map output is spilled to the disks
             (4) When this buffer reaches a threshold of occupancy, the accumulated Map outputs 
                 are merged and spilled to the disk
                   * mapred.job.shuffle.merge.percent in mapred-site.xml
                   * default = 0.66
             (5) Once the Reduce-side sort phase is over, a part of Reduce JVM heap can be used to 
                 retain Map outputs before feeding it into the final reduce function of the Reduce phase.
                   * mapred.job.reduce.input.buffer.percent in mapred-site.xml
                   * default = 0.0; means all the Reduce JVM heap is allocated to the final reduce function
                    large mapred.job.reduce.input.buffer.percent will aid in avoiding unnecessary IO 
                     operations caused by Reduce-side spills


(http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/Hadoop_Tuning_Guide-Version5.pdf)
