B. Data Disk Scaling
        Change the # of data disks
            (1) mapred.local.dir in mapred-site.xml
            (2) dfs.name.dir and dfs.data.dir in hdfs-site.xml

C. Compression
       1. Hadoop supports compression at 3 levels:
            (1) input data
            (2) intermediate Map output data
            (3) Reduce output data
       2. Hadoop supports multiple codecs that can be used for performing compression and decompression
       3. Enable Map output compression reduces the disk and network IO overhead at the expense of CPU 
          cycles necessary for compression and decompression.

        Related parameters in mapred-site.xml:
             (1) mapred.compress.map.output
             (2) mapred.map.output.compression.codec
             (3) mapred.output.compress
             (4) mapred.output.compression.type
             (5) mapred.output.compression.codec

D. JVM Reuse Policy
       1. mapred.job.reuse.jvm.num.tasks in mapred-site.xml
             The parameter governs whether Map/Reduce JVM processes spawned once are re-used for running 
              more than 1 task.
             Default value = 1 
                  * 1 means that the JVM is not re-used for running multiple tasks
                  * setting this value to -1 means an unlimited number of tasks can be scheduled on a 
                    particular JVM instance
        2. Enable JVM reuse policy reduces the overhead of JVM startup and teardown
        3. Specifically benefit in scenarios where there are large number of very short running tasks.
        4. Example: 2% improvement in performance by enabling JVM reuse.

E.HDFS Block Size
        1. Each Map task operates on an input split.
         Parameters decide the size of the input split:
            (1) mapred.min.split.size in mapred-site.xml
            (2) mapred.max.split.size in mapred-site.xml
            (3) dfs.block.size in hdfs-site.xml
        2. The input split size and the total input data size of the Hadoop workload determines
           the total number of Map tasks spawned.
        3. If the Hadoop job is spawning a large number of Map tasks experiment with large HDFS block sizes.
              Reducing the # of Map tasks can:
                  - decrease the overhead of starting up and tearing down of Map JVMs.
                  - reduce the cost in merging map output segments during the Reduce phase
                  - large block size can increase the execution time taken by each Map task, it’s better to 
                    run small amount of longer running Map tasks rather than large number of very short 
                    running Map tasks.
        4. If Map output size of proportional to the HDFS block size
              The bigger block size could lead to additional Map-side spills


(http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/Hadoop_Tuning_Guide-Version5.pdf)
